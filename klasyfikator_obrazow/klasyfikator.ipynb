{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82Zk9-9hzMr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TK1WNO7NYbVl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torchvision import transforms\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0urkKRC1h3kU"
      },
      "source": [
        "# Getting trainsets from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g30zDzgzY9-t",
        "outputId": "881ddccf-65f6-42c9-ef40-bc9d71094433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCS95dXkqhBJ"
      },
      "outputs": [],
      "source": [
        "!unzip './drive/MyDrive/cnn/train.zip'\n",
        "\n",
        "!unzip './drive/MyDrive/cnn/test_all.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8IivQMPDqurA"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wijPtsMCiB-w"
      },
      "source": [
        "# Preparations for GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RGCWfBJMg_Um"
      },
      "outputs": [],
      "source": [
        "torch.cuda.set_device(0)\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwbQNu_7S79"
      },
      "source": [
        "# Getting test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rqOgC058Y2b",
        "outputId": "65746d24-17d6-43fa-9f61-8711dad8b085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BbZzIH5E7V_a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import natsort\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class TestDataSet(Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        all_imgs = os.listdir(main_dir)\n",
        "        self._all_images = natsort.natsorted(all_imgs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self._all_images[idx]\n",
        "        img_path = os.path.join(self.main_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qUOoEeiQYbVm"
      },
      "outputs": [],
      "source": [
        "SEED = 42;\n",
        "\n",
        "transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "train_to_valid = 0.8\n",
        "\n",
        "images = torchvision.datasets.ImageFolder(\"./train/\", transform=transform)\n",
        "\n",
        "valid_index = int(train_to_valid * len(images))\n",
        "\n",
        "trainset, validset = torch.utils.data.random_split(images, [valid_index, len(images) - valid_index], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = TestDataSet(\"./test_all/\", transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZnzQS9ExYbVo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## Warstwa konwolucyjna\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn2d1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
        "        self.bn2d2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
        "        self.bn2d3 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
        "        self.bn2d4 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        ## Warstwy max pooling \n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(2304, 5000)\n",
        "        self.bn1 = nn.BatchNorm1d(5000)\n",
        "        self.d1 = nn.Dropout(0.7)\n",
        "\n",
        "        self.fc2 = nn.Linear(5000, 5000)\n",
        "        self.bn2 = nn.BatchNorm1d(5000)\n",
        "        self.d2 = nn.Dropout(0.7)\n",
        "\n",
        "        self.fc3 = nn.Linear(5000, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn2d1(F.relu(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.bn2d2(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = self.bn2d3(F.relu(self.conv3(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.bn2d4(F.relu(self.conv4(x)))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        \n",
        "        x = self.bn1(F.relu(self.fc1(x)))\n",
        "        x = self.d1(x)\n",
        "        \n",
        "        x = self.bn2(F.relu(self.fc2(x)))\n",
        "        x = self.d2(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4aEtlKcZYbVo"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARVdqr35YbVp",
        "outputId": "bf25779c-2fac-447a-d41d-2c8190bc0ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/15] loss: 0.914\n",
            "[2/15] loss: 0.766\n",
            "[3/15] loss: 0.697\n",
            "[4/15] loss: 0.640\n",
            "[5/15] loss: 0.597\n",
            "[6/15] loss: 0.559\n",
            "[7/15] loss: 0.529\n",
            "[8/15] loss: 0.493\n",
            "[9/15] loss: 0.463\n",
            "[10/15] loss: 0.432\n",
            "[11/15] loss: 0.406\n",
            "[12/15] loss: 0.372\n",
            "[13/15] loss: 0.341\n",
            "[14/15] loss: 0.314\n",
            "[15/15] loss: 0.283\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "net.train()\n",
        "\n",
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(EPOCHS): \n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    print('[%d/%d] loss: %.3f' %\n",
        "          (epoch+1 , EPOCHS,  running_loss / 2000))\n",
        "    running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS8e-ukE2dDv"
      },
      "source": [
        "## Testing on train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9gVNEJm2jCW",
        "outputId": "adf9a6f3-fdaf-4a0f-94bb-131c60297345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 86 %\n"
          ]
        }
      ],
      "source": [
        "net.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # calculate outputs by running images through the network \n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as \"prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpkDYXCr2V03"
      },
      "source": [
        "## Testing on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObeB7_6nrM4L",
        "outputId": "1c479604-6246-4860-dac8-a0217e537099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 51 %\n",
            "Accuracy for class 0 is: 39.2 %\n",
            "Accuracy for class 1 is: 48.5 %\n",
            "Accuracy for class 2 is: 40.6 %\n",
            "Accuracy for class 3 is: 43.3 %\n",
            "Accuracy for class 4 is: 44.4 %\n",
            "Accuracy for class 5 is: 56.0 %\n",
            "Accuracy for class 6 is: 60.5 %\n",
            "Accuracy for class 7 is: 45.8 %\n",
            "Accuracy for class 8 is: 33.8 %\n",
            "Accuracy for class 9 is: 50.0 %\n",
            "Accuracy for class 10 is: 36.8 %\n",
            "Accuracy for class 11 is: 66.5 %\n",
            "Accuracy for class 12 is: 61.6 %\n",
            "Accuracy for class 13 is: 28.4 %\n",
            "Accuracy for class 14 is: 59.4 %\n",
            "Accuracy for class 15 is: 46.5 %\n",
            "Accuracy for class 16 is: 33.2 %\n",
            "Accuracy for class 17 is: 54.4 %\n",
            "Accuracy for class 18 is: 41.6 %\n",
            "Accuracy for class 19 is: 32.6 %\n",
            "Accuracy for class 20 is: 69.7 %\n",
            "Accuracy for class 21 is: 56.1 %\n",
            "Accuracy for class 22 is: 83.7 %\n",
            "Accuracy for class 23 is: 47.9 %\n",
            "Accuracy for class 24 is: 61.1 %\n",
            "Accuracy for class 25 is: 43.2 %\n",
            "Accuracy for class 26 is: 75.4 %\n",
            "Accuracy for class 27 is: 43.5 %\n",
            "Accuracy for class 28 is: 47.8 %\n",
            "Accuracy for class 29 is: 63.5 %\n",
            "Accuracy for class 30 is: 55.8 %\n",
            "Accuracy for class 31 is: 52.0 %\n",
            "Accuracy for class 32 is: 31.2 %\n",
            "Accuracy for class 33 is: 62.1 %\n",
            "Accuracy for class 34 is: 51.5 %\n",
            "Accuracy for class 35 is: 42.5 %\n",
            "Accuracy for class 36 is: 54.6 %\n",
            "Accuracy for class 37 is: 66.3 %\n",
            "Accuracy for class 38 is: 28.5 %\n",
            "Accuracy for class 39 is: 41.9 %\n",
            "Accuracy for class 40 is: 59.3 %\n",
            "Accuracy for class 41 is: 62.5 %\n",
            "Accuracy for class 42 is: 47.9 %\n",
            "Accuracy for class 43 is: 52.4 %\n",
            "Accuracy for class 44 is: 48.9 %\n",
            "Accuracy for class 45 is: 58.8 %\n",
            "Accuracy for class 46 is: 48.5 %\n",
            "Accuracy for class 47 is: 68.4 %\n",
            "Accuracy for class 48 is: 56.1 %\n",
            "Accuracy for class 49 is: 28.2 %\n"
          ]
        }
      ],
      "source": [
        "net.eval()\n",
        "\n",
        "correct_pred = {classname: 0 for classname in range(50)}\n",
        "total_pred = {classname: 0 for classname in range(50)}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in validloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # calculate outputs by running images through the network \n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as \"prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "            if label == prediction:\n",
        "                correct_pred[label.item()] += 1\n",
        "            total_pred[label.item()] += 1\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:d} is: {:.1f} %\".format(classname, \n",
        "                                                   accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y73uEMq_Mx7"
      },
      "source": [
        "## Predicting test_all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHcBvETqBV_A"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Aix3Kdu7DVM"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in testset:\n",
        "    image, image_name = data\n",
        "    image = image.to(device)\n",
        "    image = image.unsqueeze(dim = 0)\n",
        "    output = net(image).squeeze()\n",
        "    _, predicted = torch.max(output.data, 0)\n",
        "    all_preds.append([image_name, predicted.item()])\n",
        "\n",
        "pd.DataFrame(all_preds).to_csv(\"results.csv\", index=False, header=None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
